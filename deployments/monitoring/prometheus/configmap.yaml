apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  deployment.rules: |
    groups:
      - name: Deployment
        rules:
          - alert: Deployment Generation Mismatch
            annotations:
              summary: Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment }} does not match, this indicates that the Deployment has failed but has not been rolled back.
            expr: |
              kube_deployment_status_observed_generation{job="kube-state-metrics"}
                !=
              kube_deployment_metadata_generation{job="kube-state-metrics"}
            for: 15m
            labels:
              severity: critical

          - alert: Deployment Replicas Mismatch
            annotations:
              summary: Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer than an hour.
            expr: |
              kube_deployment_spec_replicas{job="kube-state-metrics"}
                !=
              kube_deployment_status_replicas_available{job="kube-state-metrics"}
            for: 1h
            labels:
              severity: critical

          - alert: Deployment at 0 Replicas
            annotations:
              summary: Deployment {{$labels.deployment}} in {{$labels.namespace}} namespace currently has no pods running
            expr: |
              sum(kube_deployment_status_replicas{pod_template_hash=""}) by (deployment,namespace)  < 1
            for: 1m
            labels:
              severity: critical

          - alert: HPA Scaling Limited
            annotations:
              summary: HPA named {{$labels.hpa}} in {{$labels.namespace}} namespace has reached scaling limit
            expr: |
              (sum(kube_hpa_status_condition{condition="ScalingLimited",status="true"}) by (hpa,namespace)) == 1
            for: 1m
            labels:
              severity: critical

          - alert: HPA at MaxCapacity
            annotations:
              summary: HPA named {{$labels.hpa}} in {{$labels.namespace}} namespace is running at Max Capacity
            expr: |
              ((sum(kube_hpa_spec_max_replicas) by (hpa,namespace)) - (sum(kube_hpa_status_current_replicas) by (hpa,namespace))) == 0
            for: 1m
            labels:
              severity: critical
  expressions.rules: |
    groups:
      - name: Kubernetes
        rules:
          - expr: |
              sum(rate(container_cpu_usage_seconds_total{job="kubernetes-nodes-cadvisor", image!="", container_name!="", container_name!="POD"}[5m])) by (namespace)
            record: namespace:container_cpu_usage_seconds_total:sum_rate

          - expr: |
              sum by (namespace, pod_name, container_name) (
                rate(container_cpu_usage_seconds_total{job="kubernetes-nodes-cadvisor", image!="", container_name!="", container_name!="POD"}[5m])
              )
            record: namespace_pod_name_container_name:container_cpu_usage_seconds_total:sum_rate

          - expr: |
              sum(container_memory_usage_bytes{job="kubernetes-nodes-cadvisor", image!="", container_name!="", container_name!="POD"}) by (namespace)
            record: namespace:container_memory_usage_bytes:sum

          # FIXME: NOT WORKING
          - expr: |
              sum by (namespace, label_name) (
                 sum(rate(container_cpu_usage_seconds_total{job="kubernetes-nodes-cadvisor", image!="", container_name!="", container_name!="POD"}[5m])) by (namespace, pod_name)
               * on (namespace, pod_name) group_left(label_name)
                 label_replace(kube_pod_labels{job="kube-state-metrics"}, "pod_name", "$1", "pod", "(.*)")
              )
            record: namespace_name:container_cpu_usage_seconds_total:sum_rate

          # FIXME: NOT WORKING
          - expr: |
              sum by (namespace, label_name) (
                sum(container_memory_usage_bytes{job="kubernetes-nodes-cadvisor",image!="", container_name!="", container_name!="POD"}) by (pod_name, namespace)
              * on (namespace, pod_name) group_left(label_name)
                label_replace(kube_pod_labels{job="kubernetes-service-endpoints",kubernetes_name="kube-state-metrics"}, "pod_name", "$1", "pod", "(.*)")
              )
            record: namespace_name:container_memory_usage_bytes:sum

          # FIXME: NOT WORKING
          - expr: |
              sum by (namespace, label_name) (
                sum(kube_pod_container_resource_requests_memory_bytes{job="kubernetes-service-endpoints",kubernetes_name="kube-state-metrics"} * on (endpoint, instance, job, namespace, pod, service) group_left(phase) (kube_pod_status_phase{phase=~"^(Pending|Running)$"} == 1)) by (namespace, pod)
              * on (namespace, pod) group_left(label_name)
                label_replace(kube_pod_labels{job="kubernetes-service-endpoints",kubernetes_name="kube-state-metrics"}, "pod_name", "$1", "pod", "(.*)")
              )
            record: namespace_name:kube_pod_container_resource_requests_memory_bytes:sum

          # FIXME: NOT WORKING
          - expr: |
              sum by (namespace, label_name) (
                sum(kube_pod_container_resource_requests_cpu_cores{job="kubernetes-service-endpoints",kubernetes_name="kube-state-metrics"} * on (endpoint, instance, job, namespace, pod, service) group_left(phase) (kube_pod_status_phase{phase=~"^(Pending|Running)$"} == 1)) by (namespace, pod)
              * on (namespace, pod) group_left(label_name)
                label_replace(kube_pod_labels{job="kubernetes-service-endpoints",kubernetes_name="kube-state-metrics"}, "pod_name", "$1", "pod", "(.*)")
              )
            record: namespace_name:kube_pod_container_resource_requests_cpu_cores:sum

      - name: Kubernetes Scheduler
        rules:
          - expr: |
              histogram_quantile(0.99, sum(rate(scheduler_e2e_scheduling_latency_microseconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod)) / 1e+06
            labels:
              quantile: "0.99"
            record: cluster_quantile:scheduler_e2e_scheduling_latency:histogram_quantile

          - expr: |
              histogram_quantile(0.99, sum(rate(scheduler_scheduling_algorithm_latency_microseconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod)) / 1e+06
            labels:
              quantile: "0.99"
            record: cluster_quantile:scheduler_scheduling_algorithm_latency:histogram_quantile

          - expr: |
              histogram_quantile(0.99, sum(rate(scheduler_binding_latency_microseconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod)) / 1e+06
            labels:
              quantile: "0.99"
            record: cluster_quantile:scheduler_binding_latency:histogram_quantile

          - expr: |
              histogram_quantile(0.9, sum(rate(scheduler_e2e_scheduling_latency_microseconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod)) / 1e+06
            labels:
              quantile: "0.9"
            record: cluster_quantile:scheduler_e2e_scheduling_latency:histogram_quantile

          - expr: |
              histogram_quantile(0.9, sum(rate(scheduler_scheduling_algorithm_latency_microseconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod)) / 1e+06
            labels:
              quantile: "0.9"
            record: cluster_quantile:scheduler_scheduling_algorithm_latency:histogram_quantile

          - expr: |
              histogram_quantile(0.9, sum(rate(scheduler_binding_latency_microseconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod)) / 1e+06
            labels:
              quantile: "0.9"
            record: cluster_quantile:scheduler_binding_latency:histogram_quantile

          - expr: |
              histogram_quantile(0.5, sum(rate(scheduler_e2e_scheduling_latency_microseconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod)) / 1e+06
            labels:
              quantile: "0.5"
            record: cluster_quantile:scheduler_e2e_scheduling_latency:histogram_quantile

          - expr: |
              histogram_quantile(0.5, sum(rate(scheduler_scheduling_algorithm_latency_microseconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod)) / 1e+06
            labels:
              quantile: "0.5"
            record: cluster_quantile:scheduler_scheduling_algorithm_latency:histogram_quantile

          - expr: |
              histogram_quantile(0.5, sum(rate(scheduler_binding_latency_microseconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod)) / 1e+06
            labels:
              quantile: "0.5"
            record: cluster_quantile:scheduler_binding_latency:histogram_quantile

      - name: Kubernetes API Server
        rules:
          - expr: |
              histogram_quantile(0.99, sum(rate(apiserver_request_latencies_bucket{job="kubernetes-apiservers"}[5m])) without(instance, pod)) / 1e+06
            labels:
              quantile: "0.99"
            record: cluster_quantile:apiserver_request_latencies:histogram_quantile

          - expr: |
              histogram_quantile(0.9, sum(rate(apiserver_request_latencies_bucket{job="kubernetes-apiservers"}[5m])) without(instance, pod)) / 1e+06
            labels:
              quantile: "0.9"
            record: cluster_quantile:apiserver_request_latencies:histogram_quantile

          - expr: |
              histogram_quantile(0.5, sum(rate(apiserver_request_latencies_bucket{job="kubernetes-apiservers"}[5m])) without(instance, pod)) / 1e+06
            labels:
              quantile: "0.5"
            record: cluster_quantile:apiserver_request_latencies:histogram_quantile

      - name: Nodes
        rules:
          - expr: time() - node_boot_time_seconds{job="node-exporter"}
            record: ':node_uptime:'

          - expr: sum(min(kube_pod_info) by (node))
            record: ':kube_pod_info_node_count:'

          - expr: |
              max(label_replace(kube_pod_info{job="kube-state-metrics"}, "pod", "$1", "pod", "(.*)")) by (node, namespace, pod)
            record: 'node_namespace_pod:kube_pod_info:'

          - expr: |
              count by (node) (sum by (node, cpu) (
                node_cpu_seconds_total{job="node-exporter"}
              * on (namespace, pod) group_left(node)
                node_namespace_pod:kube_pod_info:
              ))
            record: node:node_num_cpu:sum

          - expr: |
              1 - avg(rate(node_cpu_seconds_total{job="node-exporter",mode="idle"}[1m]))
            record: :node_cpu_utilisation:avg1m

          - expr: |
              1 - avg by (node) (
                rate(node_cpu_seconds_total{job="node-exporter",mode="idle"}[1m])
              * on (namespace, pod) group_left(node)
                node_namespace_pod:kube_pod_info:)
            record: node:node_cpu_utilisation:avg1m

          - expr: |
              node:node_cpu_utilisation:avg1m
                *
              node:node_num_cpu:sum
                /
              scalar(sum(node:node_num_cpu:sum))
            record: node:cluster_cpu_utilisation:ratio

          - expr: |
              sum(node_load1{job="node-exporter"})
              /
              sum(node:node_num_cpu:sum)
            record: ':node_cpu_saturation_load1:'

          - expr: |
              sum by (node) (
                node_load1{job="node-exporter"}
              * on (namespace, pod) group_left(node)
                node_namespace_pod:kube_pod_info:
              )
              /
              node:node_num_cpu:sum
            record: 'node:node_cpu_saturation_load1:'

          - expr: |
              1 -
              sum(node_memory_MemFree_bytes{job="node-exporter"} + node_memory_Cached_bytes{job="node-exporter"} + node_memory_Buffers_bytes{job="node-exporter"})
              /
              sum(node_memory_MemTotal_bytes{job="node-exporter"})
            record: ':node_memory_utilisation:'

          - expr: |
              sum(node_memory_MemFree_bytes{job="node-exporter"} + node_memory_Cached_bytes{job="node-exporter"} + node_memory_Buffers_bytes{job="node-exporter"})
            record: :node_memory_MemFreeCachedBuffers_bytes:sum

          - expr: |
              sum(node_memory_MemTotal_bytes{job="node-exporter"})
            record: :node_memory_MemTotal_bytes:sum

          - expr: |
              sum by (node) (
                (node_memory_MemFree_bytes{job="node-exporter"} + node_memory_Cached_bytes{job="node-exporter"} + node_memory_Buffers_bytes{job="node-exporter"})
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
              )
            record: node:node_memory_bytes_available:sum

          - expr: |
              sum by (node) (
                node_memory_MemTotal_bytes{job="node-exporter"}
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
              )
            record: node:node_memory_bytes_total:sum

          - expr: |
              (node:node_memory_bytes_total:sum - node:node_memory_bytes_available:sum)
              /
              node:node_memory_bytes_total:sum
            record: node:node_memory_utilisation:ratio

          - expr: |
              (node:node_memory_bytes_total:sum - node:node_memory_bytes_available:sum)
              /
              scalar(sum(node:node_memory_bytes_total:sum))
            record: node:cluster_memory_utilisation:ratio

          - expr: |
              1e3 * sum(
                (rate(node_vmstat_pgpgin{job="node-exporter"}[1m])
               + rate(node_vmstat_pgpgout{job="node-exporter"}[1m]))
              )
            record: :node_memory_swap_io_bytes:sum_rate

          - expr: |
              1 -
              sum by (node) (
                (node_memory_MemFree_bytes{job="node-exporter"} + node_memory_Cached_bytes{job="node-exporter"} + node_memory_Buffers_bytes{job="node-exporter"})
              * on (namespace, pod) group_left(node)
                node_namespace_pod:kube_pod_info:
              )
              /
              sum by (node) (
                node_memory_MemTotal_bytes{job="node-exporter"}
              * on (namespace, pod) group_left(node)
                node_namespace_pod:kube_pod_info:
              )
            record: 'node:node_memory_utilisation:'

          - expr: |
              1 - (node:node_memory_bytes_available:sum / node:node_memory_bytes_total:sum)
            record: 'node:node_memory_utilisation_2:'

          - expr: |
              1e3 * sum by (node) (
                (rate(node_vmstat_pgpgin{job="node-exporter"}[1m])
               + rate(node_vmstat_pgpgout{job="node-exporter"}[1m]))
               * on (namespace, pod) group_left(node)
                 node_namespace_pod:kube_pod_info:
              )
            record: node:node_memory_swap_io_bytes:sum_rate

          - expr: |
              avg(irate(node_disk_io_time_seconds_total{job="node-exporter",device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+"}[1m]))
            record: :node_disk_utilisation:avg_irate

          - expr: |
              avg by (node) (
                irate(node_disk_io_time_seconds_total{job="node-exporter",device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+"}[1m])
              * on (namespace, pod) group_left(node)
                node_namespace_pod:kube_pod_info:
              )
            record: node:node_disk_utilisation:avg_irate

          - expr: |
              avg(irate(node_disk_io_time_weighted_seconds_total{job="node-exporter",device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+"}[1m]))
            record: :node_disk_saturation:avg_irate

          - expr: |
              avg by (node) (
                irate(node_disk_io_time_weighted_seconds_total{job="node-exporter",device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+"}[1m])
              * on (namespace, pod) group_left(node)
                node_namespace_pod:kube_pod_info:
              )
            record: node:node_disk_saturation:avg_irate

          - expr: |
              max by (namespace, pod, device) ((node_filesystem_size_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"}
              - node_filesystem_avail_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"})
              / node_filesystem_size_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"})
            record: 'node:node_filesystem_usage:'

          - expr: |
              max by (namespace, pod, device) (node_filesystem_avail_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"} / node_filesystem_size_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"})
            record: 'node:node_filesystem_avail:'

          - expr: |
              sum(irate(node_network_receive_bytes_total{job="node-exporter",device!~"veth.+"}[1m])) +
              sum(irate(node_network_transmit_bytes_total{job="node-exporter",device!~"veth.+"}[1m]))
            record: :node_net_utilisation:sum_irate

          - expr: |
              sum by (node) (
                (irate(node_network_receive_bytes_total{job="node-exporter",device!~"veth.+"}[1m]) +
                irate(node_network_transmit_bytes_total{job="node-exporter",device!~"veth.+"}[1m]))
              * on (namespace, pod) group_left(node)
                node_namespace_pod:kube_pod_info:
              )
            record: node:node_net_utilisation:sum_irate

          - expr: |
              sum(irate(node_network_receive_drop_total{job="node-exporter",device!~"veth.+"}[1m])) +
              sum(irate(node_network_transmit_drop_total{job="node-exporter",device!~"veth.+"}[1m]))
            record: :node_net_saturation:sum_irate

          - expr: |
              sum by (node) (
                (irate(node_network_receive_drop_total{job="node-exporter",device!~"veth.+"}[1m]) +
                irate(node_network_transmit_drop_total{job="node-exporter",device!~"veth.+"}[1m]))
              * on (namespace, pod) group_left(node)
                node_namespace_pod:kube_pod_info:
              )
            record: node:node_net_saturation:sum_irate

          - expr: |
              max(
                max(
                  kube_pod_info{job="kube-state-metrics", host_ip!=""}
                ) by (node, host_ip)
                * on (host_ip) group_right (node)
                label_replace(
                  (max(node_filesystem_files{job="node-exporter", mountpoint="/"}) by (instance)), "host_ip", "$1", "instance", "(.*):.*"
                )
              ) by (node)
            record: 'node:node_inodes_total:'

          - expr: |
              max(
                max(
                  kube_pod_info{job="kube-state-metrics", host_ip!=""}
                ) by (node, host_ip)
                * on (host_ip) group_right (node)
                label_replace(
                  (max(node_filesystem_files_free{job="node-exporter", mountpoint="/"}) by (instance)), "host_ip", "$1", "instance", "(.*):.*"
                )
              ) by (node)
            record: 'node:node_inodes_free:'

      - name: Prometheus Node Recording
        rules:
          - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[3m])) BY
              (instance)
            record: instance:node_cpu:rate:sum

          - expr: sum((node_filesystem_size_bytes{mountpoint="/"} - node_filesystem_free_bytes{mountpoint="/"}))
              BY (instance)
            record: instance:node_filesystem_usage:sum

          - expr: sum(rate(node_network_receive_bytes_total[3m])) BY (instance)
            record: instance:node_network_receive_bytes:rate:sum

          - expr: sum(rate(node_network_transmit_bytes_total[3m])) BY (instance)
            record: instance:node_network_transmit_bytes:rate:sum

          - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[5m])) WITHOUT
              (cpu, mode) / ON(instance) GROUP_LEFT() count(sum(node_cpu_seconds_total)
              BY (instance, cpu)) BY (instance)
            record: instance:node_cpu:ratio

          - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[5m]))
            record: cluster:node_cpu:sum_rate5m

          - expr: cluster:node_cpu_seconds_total:rate5m / count(sum(node_cpu_seconds_total)
              BY (instance, cpu))
            record: cluster:node_cpu:ratio
  general.rules: |
    groups:
      - name: General
        rules:
          - alert: Target Down
            annotations:
              summary: '{{ $value }}% of the {{ $labels.job }} targets are down.'
            expr: 100 * (count(up == 0) BY (job) / count(up) BY (job)) > 10
            for: 10m
            labels:
              severity: warning

          # - alert: Watchdog
          #   annotations:
          #     summary: |-
          #       This is an alert meant to ensure that the entire alerting pipeline is functional.
          #       This alert is always firing, therefore it should always be firing in Alertmanager
          #       and always fire against a receiver. There are integrations with various notification
          #       mechanisms that send a notification when this alert is not firing. For example the
          #       "DeadMansSnitch" integration in PagerDuty.
          #   expr: vector(1)
          #   labels:
          #     severity: none
  infrastructure.rules: |
    groups:
      - name: Infrastructure
        rules:
          - alert: Alert Manager Down
            annotations:
              summary: Alert Manager has disappeared.
            expr: |
              absent(up{job="alertmanager",namespace="monitoring"} == 1)
            for: 15m
            labels:
              severity: critical

          - alert: Core DNS Down
            annotations:
              summary: CoreDNS has disappeared.
            expr: |
              absent(up{job="kube-dns"} == 1)
            for: 15m
            labels:
              severity: critical

          - alert: Kubernetes API Server Down
            annotations:
              summary: Kubernetes API Server has disappeared.
            expr: |
              absent(up{job="kubernetes-apiservers"} == 1)
            for: 15m
            labels:
              severity: critical

          #- alert: Kubernetes Controller Manager Down
          #  annotations:
          #    summary: Kubernetes Controller Manager has disappeared.
          #  expr: |
          #    absent(up{job="kube-controller-manager"} == 1)
          #  for: 15m
          #  labels:
          #    severity: critical

          #- alert: Kubernetes Scheduler Down
          #  annotations:
          #    summary: Kubernetes Scheduler has disappeared.
          #  expr: |
          #    absent(up{job="kube-scheduler"} == 1)
          #  for: 15m
          #  labels:
          #    severity: critical

          - alert: Kube State Metrics Down
            annotations:
              summary: Kube State Metrics has disappeared.
            expr: |
              absent(up{job="kube-state-metrics"} == 1)
            for: 15m
            labels:
              severity: critical

          - alert: Kubelet Down
            annotations:
              summary: Kubelet has disappeared.
            expr: |
              absent(up{job="kubernetes-nodes-kubelet"} == 1)
            for: 15m
            labels:
              severity: critical

          - alert: Node Exporter Down
            annotations:
              summary: Node Exporter has disappeared.
            expr: |
              absent(up{job="node-exporter"} == 1)
            for: 15m
            labels:
              severity: critical

          # Will never actually work!
          #- alert: Prometheus Down
          #  annotations:
          #    summary: Prometheus has disappeared.
          #  expr: |
          #    absent(up{job="prometheus", namespace="monitoring"} == 1)
          #  for: 15m
          #  labels:
          #    severity: critical
  kubernetes.rules: |
    groups:
      - name: Kubernetes Apps
        rules:
          - alert: StatefulSet Replicas Mismatch
            annotations:
              summary: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not matched the expected number of replicas for longer than 15 minutes.
            expr: |
              kube_statefulset_status_replicas_ready{job="kube-state-metrics"}
                !=
              kube_statefulset_status_replicas{job="kube-state-metrics"}
            for: 15m
            labels:
              severity: critical

          - alert: StatefulSet Generation Mismatch
            annotations:
              summary: StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset }} does not match, this indicates that the StatefulSet has failed but has not been rolled back.
            expr: |
              kube_statefulset_status_observed_generation{job="kube-state-metrics"}
                !=
              kube_statefulset_metadata_generation{job="kube-state-metrics"}
            for: 15m
            labels:
              severity: critical

          - alert: StatefulSet Update Not Rolled Out
            annotations:
              summary: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update has not been rolled out.
            expr: |
              max without (revision) (
                kube_statefulset_status_current_revision{job="kube-state-metrics"}
                  unless
                kube_statefulset_status_update_revision{job="kube-state-metrics"}
              )
                *
              (
                kube_statefulset_replicas{job="kube-state-metrics"}
                  !=
                kube_statefulset_status_replicas_updated{job="kube-state-metrics"}
              )
            for: 15m
            labels:
              severity: critical

          - alert: DaemonSet Rollout Stuck
            annotations:
              summary: Only {{ $value }}% of the desired Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are scheduled and ready.
            expr: |
              kube_daemonset_status_number_ready{job="kube-state-metrics"}
                /
              kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"} * 100 < 100
            for: 15m
            labels:
              severity: critical

          - alert: DaemonSet Not Scheduled
            annotations:
              summary: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled.'
            expr: |
              kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}
                -
              kube_daemonset_status_current_number_scheduled{job="kube-state-metrics"} > 0
            for: 10m
            labels:
              severity: warning

          - alert: Daemon Set Mis-scheduled
            annotations:
              summary: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are running where they are not supposed to run.'
            expr: |
              kube_daemonset_status_number_misscheduled{job="kube-state-metrics"} > 0
            for: 10m
            labels:
              severity: warning

          - alert: CronJob Running
            annotations:
              summary: CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is taking more than 1h to complete.
            expr: |
              time() - kube_cronjob_next_schedule_time{job="kube-state-metrics"} > 3600
            for: 1h
            labels:
              severity: warning

          - alert: Job Completion
            annotations:
              summary: Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more than one hour to complete.
            expr: |
              kube_job_spec_completions{job="kube-state-metrics"} - kube_job_status_succeeded{job="kube-state-metrics"}  > 0
            for: 1h
            labels:
              severity: warning

          - alert: Job Failed
            annotations:
              summary: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete.
            expr: |
              kube_job_status_failed{job="kube-state-metrics"}  > 0
            for: 1h
            labels:
              severity: warning

      - name: Kubernetes Resources
        rules:
          - alert:  CPU Over Commit
            annotations:
              summary: Cluster has overcommitted CPU resource requests for Pods and cannot tolerate node failure.
            expr: |
              sum(namespace_name:kube_pod_container_resource_requests_cpu_cores:sum)
                /
              sum(node:node_num_cpu:sum)
                >
              (count(node:node_num_cpu:sum)-1) / count(node:node_num_cpu:sum)
            for: 5m
            labels:
              severity: warning

          - alert: Memory Over Commit
            annotations:
              summary: Cluster has overcommitted memory resource requests for Pods and cannot tolerate node failure.
            expr: |
              sum(namespace_name:kube_pod_container_resource_requests_memory_bytes:sum)
                /
              sum(node_memory_MemTotal_bytes)
                >
              (count(node:node_num_cpu:sum)-1)
                /
              count(node:node_num_cpu:sum)
            for: 5m
            labels:
              severity: warning

          - alert: Memory Over Commit
            annotations:
              summary: Cluster has overcommitted CPU resource requests for Namespaces.
            expr: |
              sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="cpu"})
                /
              sum(node:node_num_cpu:sum)
                > 1.5
            for: 5m
            labels:
              severity: warning

          - alert: Memory Over Commit
            annotations:
              summary: Cluster has overcommitted memory resource requests for Namespaces.
            expr: |
              sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="memory"})
                /
              sum(node_memory_MemTotal_bytes{job="node-exporter"})
                > 1.5
            for: 5m
            labels:
              severity: warning

          - alert: Quota Exceeded
            annotations:
              summary: Namespace {{ $labels.namespace }} is using {{ printf "%0.0f" $value }}% of its {{ $labels.resource }} quota.
            expr: |
              100 * kube_resourcequota{job="kube-state-metrics", type="used"}
                / ignoring(instance, job, type)
              (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
                > 90
            for: 15m
            labels:
              severity: warning

          - alert: CPU Throttling High
            annotations:
              summary: '{{ printf "%0.0f" $value }}% throttling of CPU in namespace {{ $labels.namespace }} for container {{ $labels.container_name }} in pod {{ $labels.pod_name }}.'
            expr: |
              100 * sum(increase(container_cpu_cfs_throttled_periods_total{container_name!="",}[5m])) by (container_name, pod_name, namespace)  / sum(increase(container_cpu_cfs_periods_total{}[5m]))
              by (container_name, pod_name, namespace) > 25
            for: 15m
            labels:
              severity: warning

      - name: Kubernetes Storage
        rules:
          - alert: Persistent Volume Usage Critical
            annotations:
              summary: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim
                }} in Namespace {{ $labels.namespace }} is only {{ printf "%0.2f" $value
                }}% free.
            expr: |
              100 * kubelet_volume_stats_available_bytes{job="kubelet"}
                /
              kubelet_volume_stats_capacity_bytes{job="kubelet"}
                < 3
            for: 1m
            labels:
              severity: critical

          - alert: Persistent Volume Full in Four Days
            annotations:
              summary: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim
                }} in Namespace {{ $labels.namespace }} is expected to fill up within four
                days. Currently {{ printf "%0.2f" $value }}% is available.
            expr: |
              100 * (
                kubelet_volume_stats_available_bytes{job="kubelet"}
                  /
                kubelet_volume_stats_capacity_bytes{job="kubelet"}
              ) < 15
              and
              predict_linear(kubelet_volume_stats_available_bytes{job="kubelet"}[6h], 4 * 24 * 3600) < 0
            for: 5m
            labels:
              severity: critical

          - alert: Persistent Volume Errors
            annotations:
              summary: The persistent volume {{ $labels.persistentvolume }} has status {{ $labels.phase }}.
            expr: |
              kube_persistentvolume_status_phase{phase=~"Failed|Pending", job="kube-state-metrics"} > 0
            for: 5m
            labels:
              severity: critical

      - name: Kubernetes System
        rules:
          - alert: Version Mismatch
            annotations:
              summary: There are {{ $value }} different semantic versions of Kubernetes components running.
            expr: |
              count(count by (gitVersion) (label_replace(kubernetes_build_info{job!="kube-dns"},"gitVersion","$1","gitVersion","(v[0-9]*.[0-9]*.[0-9]*).*"))) > 1
            for: 1h
            labels:
              severity: warning

          - alert: Client Errors
            annotations:
              message: Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ printf "%0.0f" $value }}% errors.'
            expr: |
              (sum(rate(rest_client_requests_total{code=~"5.."}[5m])) by (instance, job)
                /
              sum(rate(rest_client_requests_total[5m])) by (instance, job))
              * 100 > 1
            for: 15m
            labels:
              severity: warning

          - alert: Client Errors
            annotations:
              summary: Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ printf "%0.0f" $value }} errors / second.
            expr: |
              sum(rate(ksm_scrape_error_total{job="kube-state-metrics"}[5m])) by (instance, job) > 0.1
            for: 15m
            labels:
              severity: warning

          - alert: API Latency High
            annotations:
              summary: The API server has a 99th percentile latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}.
            expr: |
              cluster_quantile:apiserver_request_latencies:histogram_quantile{job="kubernetes-apiservers",quantile="0.99",subresource!="log",verb!~"^(?:LIST|WATCH|WATCHLIST|PROXY|CONNECT)$"} > 1
            for: 10m
            labels:
              severity: warning

          - alert: API Latency High
            annotations:
              summary: The API server has a 99th percentile latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}.
            expr: |
              cluster_quantile:apiserver_request_latencies:histogram_quantile{job="kubernetes-apiservers",quantile="0.99",subresource!="log",verb!~"^(?:LIST|WATCH|WATCHLIST|PROXY|CONNECT)$"} > 4
            for: 10m
            labels:
              severity: critical

          - alert: API Errors High
            annotations:
              summary: API server is returning errors for {{ $value }}% of requests.
            expr: |
              sum(rate(apiserver_request_count{job="kubernetes-apiservers",code=~"^(?:5..)$"}[5m]))
                /
              sum(rate(apiserver_request_count{job="kubernetes-apiservers"}[5m])) * 100 > 3
            for: 10m
            labels:
              severity: critical

          - alert: API Errors High
            annotations:
              summary: API server is returning errors for {{ $value }}% of requests.
            expr: |
              sum(rate(apiserver_request_count{job="kubernetes-apiservers",code=~"^(?:5..)$"}[5m]))
                /
              sum(rate(apiserver_request_count{job="kubernetes-apiservers"}[5m])) * 100 > 1
            for: 10m
            labels:
              severity: warning

          - alert: API Errors High
            annotations:
              summary: API server is returning errors for {{ $value }}% of requests for {{ $labels.verb }} {{ $labels.resource }} {{ $labels.subresource }}.
            expr: |
              sum(rate(apiserver_request_count{job="kubernetes-apiservers",code=~"^(?:5..)$"}[5m])) by (resource,subresource,verb)
                /
              sum(rate(apiserver_request_count{job="kubernetes-apiservers"}[5m])) by (resource,subresource,verb) * 100 > 10
            for: 10m
            labels:
              severity: critical

          - alert: API Errors High
            annotations:
              summary: API server is returning errors for {{ $value }}% of requests for {{ $labels.verb }} {{ $labels.resource }} {{ $labels.subresource }}.
            expr: |
              sum(rate(apiserver_request_count{job="kubernetes-apiservers",code=~"^(?:5..)$"}[5m])) by (resource,subresource,verb)
                /
              sum(rate(apiserver_request_count{job="kubernetes-apiservers"}[5m])) by (resource,subresource,verb) * 100 > 5
            for: 10m
            labels:
              severity: warning

          - alert: Client Certificate Expiration
            annotations:
              summary: A client certificate used to authenticate to the apiserver is expiring in less than 7.0 days.
            expr: |
              apiserver_client_certificate_expiration_seconds_count{job="kubernetes-apiservers"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="kubernetes-apiservers"}[5m]))) < 604800
            labels:
              severity: warning

          - alert: Client Certificate Expiration
            annotations:
              summary: A client certificate used to authenticate to the apiserver is expiring in less than 24.0 hours.
            expr: |
              apiserver_client_certificate_expiration_seconds_count{job="kubernetes-apiservers"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="kubernetes-apiservers"}[5m]))) < 86400
            labels:
              severity: critical
  nodes.rules: |
    groups:
      - name: Nodes
        rules:
          - alert: High Node Memory Usage
            annotations:
              summary: Node {{$labels.kubernetes_io_hostname}} has more than 80% memory used.
            expr: |
              (sum (container_memory_working_set_bytes{id="/",container_name!="POD"}) by (kubernetes_io_hostname) / sum (machine_memory_bytes{}) by (kubernetes_io_hostname) * 100) > 80
            for: 5m
            labels:
              severity: critical

          - alert: High Node CPU Usage
            annotations:
              summary: Node {{$labels.kubernetes_io_hostname}} has more than 80% allocatable CPU used.
            expr: |
              (sum(rate(container_cpu_usage_seconds_total{id="/", container_name!="POD"}[1m])) by (kubernetes_io_hostname) / sum(machine_cpu_cores) by (kubernetes_io_hostname)  * 100) > 80
            for: 5m
            labels:
              severity: critical

          - alert: High Node Disk Usage
            annotations:
              summary: Node {{$labels.kubernetes_io_hostname}} has more than 85% disk used.
            expr: |
              (sum(container_fs_usage_bytes{device=~"^/dev/([sv]d[a-z][1-9]|root)$",id="/",container_name!="POD"}) by (kubernetes_io_hostname) / sum(container_fs_limit_bytes{container_name!="POD",device=~"^/dev/([sv]d[a-z][1-9]|root)$",id="/"}) by (kubernetes_io_hostname)) * 100 > 85
            for: 5m
            labels:
              severity: critical

          - alert: Node Disk Running Full
            annotations:
              summary: Device {{ $labels.device }} of node-exporter {{ $labels.namespace }}/{{ $labels.pod }} will be full within the next 24 hours.
            expr: |
              (node:node_filesystem_usage: > 0.85) and (predict_linear(node:node_filesystem_avail:[6h], 3600 * 24) < 0)
            for: 30m
            labels:
              severity: warning

          - alert: Node Disk Running Full
            annotations:
              summary: Device {{ $labels.device }} of node-exporter {{ $labels.namespace }}/{{ $labels.pod }} will be full within the next 2 hours.
            expr: |
              (node:node_filesystem_usage: > 0.85) and (predict_linear(node:node_filesystem_avail:[30m], 3600 * 2) < 0)
            for: 10m
            labels:
              severity: critical

          - alert: Node Not Ready
            annotations:
              summary: '{{ $labels.node }} has been unready for more than an hour.'
            expr: |
              kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"} == 0
            for: 1h
            labels:
              severity: warning
  pods.rules: |
    groups:
      - name: Pods
        rules:
          - alert: Init Containers Failed
            expr: kube_pod_init_container_status_last_terminated_reason{reason!="Completed"} == 1
            annotations:
              summary: '{{ $labels.container }} init failed'
              description: '{{ $labels.container }} has not completed init containers with the reason {{ $labels.reason }}'
            for: 10m
            labels:
              severity: critical

          - alert: Pod Crash Looping
            annotations:
              summary: Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is restarting {{ printf "%.2f" $value }} times / 5 minutes.
            expr: |
              rate(kube_pod_container_status_restarts_total{job="kube-state-metrics"}[15m]) * 60 * 5 > 0
            for: 1h
            labels:
              severity: critical

          - alert: Pod Not Ready
            annotations:
              summary: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than an hour.
            expr: |
              sum by (namespace, pod) (kube_pod_status_phase{job="kube-state-metrics", phase=~"Pending|Unknown"}) > 0
            for: 1h
            labels:
              severity: critical

          - alert: Container Restarted
            annotations:
              summary: Container named {{$labels.container}} in {{$labels.pod}} in {{$labels.namespace}} namespace was restarted
            expr: |
              sum(increase(kube_pod_container_status_restarts_total{namespace!="kube-system",pod_template_hash=""}[1m])) by (pod,namespace,container) > 0
            for: 0m
            labels:
              severity: warning

          - alert: High Memory Usage of Container
            annotations:
              summary: Container named {{$labels.container}} in {{$labels.pod}} in {{$labels.namespace}} namespace is using more than 75% of Memory Limit
            expr: |
              ((( sum(container_memory_usage_bytes{image!="",container_name!="POD", namespace!="kube-system"}) by (namespace,container_name,pod_name)  / sum(container_spec_memory_limit_bytes{image!="",container_name!="POD",namespace!="kube-system"}) by (namespace,container_name,pod_name) ) * 100 ) < +Inf ) > 75
            for: 5m
            labels:
              severity: warning

          - alert: High CPU Usage of Container
            annotations:
              summary: Container named {{$labels.container}} in {{$labels.pod}} in {{$labels.namespace}} namespace is using more than 75% of CPU Limit
            expr: |
              ((sum(irate(container_cpu_usage_seconds_total{image!="",container_name!="POD", namespace!="kube-system"}[30s])) by (namespace,container_name,pod_name) / sum(container_spec_cpu_quota{image!="",container_name!="POD", namespace!="kube-system"} / container_spec_cpu_period{image!="",container_name!="POD", namespace!="kube-system"}) by (namespace,container_name,pod_name) ) * 100)  > 75
            for: 5m
            labels:
              severity: warning

          - alert: Too Many Pods
            annotations:
              summary: Kubelet {{ $labels.instance }} is running {{ $value }} Pods, close to the limit of 110.
            expr: |
              kubelet_running_pod_count{job="kubelet"} > 110 * 0.9
            for: 15m
            labels:
              severity: warning
  prometheus.rules: |
    groups:
      - name: Prometheus
        rules:
          - alert: Prometheus Config Reload Failed
            annotations:
              description: Reloading Prometheus' configuration has failed for {{ $labels.namespace }}/{{ $labels.pod }}
              summary: Reloading Prometheus' configuration failed
            expr: |
              prometheus_config_last_reload_successful{job="prometheus",namespace="monitoring"} == 0
            for: 10m
            labels:
              severity: warning

          - alert: Prometheus Notification Queue Running Full
            annotations:
              description: Prometheus' alert notification queue is running full for {{$labels.namespace}}/{{$labels.pod}}
              summary: Prometheus' alert notification queue is running full
            expr: |
              predict_linear(prometheus_notifications_queue_length{job="prometheus",namespace="monitoring"}[5m], 60 * 30) > prometheus_notifications_queue_capacity{job="prometheus",namespace="monitoring"}
            for: 10m
            labels:
              severity: warning

          - alert: Prometheus Error Sending Alerts
            annotations:
              description: Errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.Alertmanager}}
              summary: Errors while sending alert from Prometheus
            expr: |
              rate(prometheus_notifications_errors_total{job="prometheus",namespace="monitoring"}[5m]) / rate(prometheus_notifications_sent_total{job="prometheus",namespace="monitoring"}[5m]) > 0.01
            for: 10m
            labels:
              severity: warning

          - alert: Prometheus Error Sending Alerts
            annotations:
              description: Errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.Alertmanager}}
              summary: Errors while sending alerts from Prometheus
            expr: |
              rate(prometheus_notifications_errors_total{job="prometheus",namespace="monitoring"}[5m]) / rate(prometheus_notifications_sent_total{job="prometheus",namespace="monitoring"}[5m]) > 0.03
            for: 10m
            labels:
              severity: critical

          - alert: Prometheus not Connected to Alert Manager
            annotations:
              description: Prometheus {{ $labels.namespace }}/{{ $labels.pod}} is not connected to any Alert managers
              summary: Prometheus is not connected to any Alert managers
            expr: |
              prometheus_notifications_alertmanagers_discovered{job="prometheus",namespace="monitoring"} < 1
            for: 10m
            labels:
              severity: warning

          - alert: Prometheus TSDB Reloads Failing
            annotations:
              description: '{{$labels.job}} at {{$labels.instance}} had {{$value | humanize }} reload failures over the last four hours.'
              summary: Prometheus has issues reloading data blocks from disk
            expr: |
              increase(prometheus_tsdb_reloads_failures_total{job="prometheus",namespace="monitoring"}[2h]) > 0
            for: 12h
            labels:
              severity: warning

          - alert: Prometheus TSDB Compactions Failing
            annotations:
              description: '{{$labels.job}} at {{$labels.instance}} had {{$value | humanize }} compaction failures over the last four hours.'
              summary: Prometheus has issues compacting sample blocks
            expr: |
              increase(prometheus_tsdb_compactions_failed_total{job="prometheus",namespace="monitoring"}[2h]) > 0
            for: 12h
            labels:
              severity: warning

          - alert: Prometheus TSDB WAL Corruptions
            annotations:
              description: '{{$labels.job}} at {{$labels.instance}} has a corrupted write-ahead log (WAL).'
              summary: Prometheus write-ahead log is corrupted
            expr: |
              prometheus_tsdb_wal_corruptions_total{job="prometheus",namespace="monitoring"} > 0
            for: 4h
            labels:
              severity: warning

          - alert: Prometheus Not Ingesting Samples
            annotations:
              description: Prometheus {{ $labels.namespace }}/{{ $labels.pod}} isn't ingesting samples.
              summary: Prometheus isn't ingesting samples
            expr: |
              rate(prometheus_tsdb_head_samples_appended_total{job="prometheus",namespace="monitoring"}[5m]) <= 0
            for: 10m
            labels:
              severity: warning

          - alert: Prometheus Target Scrapes Duplicate
            annotations:
              description: '{{$labels.namespace}}/{{$labels.pod}} has many samples rejected due to duplicate timestamps but different values'
              summary: Prometheus has many samples rejected
            expr: |
              increase(prometheus_target_scrapes_sample_duplicate_timestamp_total{job="prometheus",namespace="monitoring"}[5m]) > 0
            for: 10m
            labels:
              severity: warning

      - name: Alert Manager
        rules:
          - alert: Alert Manager Failed Reload
            annotations:
              message: Reloading Alertmanager's configuration has failed for {{ $labels.namespace }}/{{ $labels.pod}}.
            expr: |
              alertmanager_config_last_reload_successful{job="alertmanager",namespace="monitoring"} == 0
            for: 10m
            labels:
              severity: warning
  prometheus.yaml: |
    global:
      scrape_interval: 60s
      evaluation_interval: 60s

    rule_files:
      - "/etc/prometheus/*.rules"

    alerting:
      alertmanagers:
        - scheme: http
          path_prefix: /
          static_configs:
            - targets: ['alertmanager.monitoring.svc:9093']

    scrape_configs:
      - job_name: traefik
        static_configs:
          - targets: ['traefik-ingress-controller.infra.svc:8080']

      # FIXME: This does not work
      # - job_name: etcd
      #  static_configs:
      #      - targets: ['etcd-master.kube-system.svc:4001']

      - job_name: nodes
        static_configs:
          - targets:
              - '10.0.0.1:9100'
              - '10.0.0.2:9100'
              - '10.0.0.3:9100'
              - '10.0.0.4:9100'

      # - job_name: vault
      #   static_configs:
      #      - targets: ['vault.vault.svc:8200']
      #   metrics_path: "/v1/sys/metrics"
      #   params:
      #     format: ['prometheus']
      #   bearer_token: s.brVZ623SwKfiYg1WjVWqDcS6

      - job_name: prometheus
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - action: keep
            source_labels: [__meta_kubernetes_service_label_name]
            regex: prometheus
          - action: keep
            source_labels: [__meta_kubernetes_endpoint_port_name]
            regex: prometheus
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_service_name]
            target_label: service
          - source_labels: [__meta_kubernetes_service_name]
            target_label: job
            replacement: ${1}
          - target_label: endpoint
            replacement: prometheus

      - job_name: alertmanager
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names:
                - monitoring
        relabel_configs:
          - action: keep
            source_labels: [__meta_kubernetes_service_label_name]
            regex: alertmanager
          - action: keep
            source_labels: [__meta_kubernetes_endpoint_port_name]
            regex: alertmanager
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_service_name]
            target_label: service
          - source_labels: [__meta_kubernetes_service_name]
            target_label: job
            replacement: ${1}
          - target_label: endpoint
            replacement: alertmanager

      # Scrape config for API servers.
      #
      # Kubernetes exposes API servers as endpoints to the default/kubernetes
      # service so this uses `endpoints` role and uses relabelling to only keep
      # the endpoints associated with the default/kubernetes service using the
      # default named port `https`. This works for single API server deployments as
      # well as HA API server deployments.
      - job_name: kubernetes-apiservers
        kubernetes_sd_configs:
          - role: endpoints
        # Default to scraping over https. If required, just disable this or change to `http`.
        scheme: https
        # Keep only the default/kubernetes service endpoints for the https port. This
        # will add targets for each API server which Kubernetes adds an endpoint to
        # the default/kubernetes service.
        relabel_configs:
          - action: keep
            regex: default;kubernetes;https
            source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
        # This TLS & bearer token file config is used to connect to the actual scrape
        # endpoints for cluster components. This is separate to discovery auth
        # configuration because discovery & scraping are two separate concerns in
        # Prometheus. The discovery auth config is automatic if Prometheus runs inside
        # the cluster. Otherwise, more config options have to be provided within the
        # <kubernetes_sd_config>.
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          # If your node certificates are self-signed or use a different CA to the
          # master CA, then disable certificate verification below. Note that
          # certificate verification is an integral part of a secure infrastructure
          # so this should only be disabled in a controlled environment. You can
          # disable certificate verification by uncommenting the line below.
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      # Scrape config for nodes (kubelet).
      #
      # Rather than connecting directly to the node, the scrape is proxied though the
      # Kubernetes apiserver.  This means it will work if Prometheus is running out of
      # cluster, or can't connect to nodes for some other reason (e.g. because of
      # firewalling).
      - job_name: kubernetes-nodes-kubelet
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          # - target_label: __address__
          #   replacement: kubernetes.default.svc:443
          # - source_labels: [__meta_kubernetes_node_name]
          #   regex: (.+)
          #   target_label: __metrics_path__
          #   replacement: /api/v1/nodes/${1}/proxy/metrics
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      # Scrape config for Kubelet cAdvisor.
      #
      # This is required for Kubernetes 1.7.3 and later, where cAdvisor metrics
      # (those whose names begin with 'container_') have been removed from the
      # Kubelet metrics endpoint.  This job scrapes the cAdvisor endpoint to
      # retrieve those metrics.
      #
      # In Kubernetes 1.7.0-1.7.2, these metrics are only exposed on the cAdvisor
      # HTTP endpoint; use "replacement: /api/v1/nodes/${1}:4194/proxy/metrics"
      # in that case (and ensure cAdvisor's HTTP server hasn't been disabled with
      # the --cadvisor-port=0 Kubelet flag).
      #
      # This job is not necessary and should be removed in Kubernetes 1.6 and
      # earlier versions, or it will cause the metrics to be scraped twice.
      - job_name: kubernetes-nodes-cadvisor
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
        metric_relabel_configs:
          - action: replace
            source_labels: [id]
            regex: '^/machine\.slice/machine-rkt\\x2d([^\\]+)\\.+/([^/]+)\.service$'
            target_label: rkt_container_name
            replacement: '${2}-${1}'
          - action: replace
            source_labels: [id]
            regex: '^/system\.slice/(.+)\.service$'
            target_label: systemd_service_name
            replacement: '${1}'
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      # Scrape config for service endpoints.
      #
      # The relabeling allows the actual service scrape endpoint to be configured
      # via the following annotations:
      #
      # * `prometheus.io/scrape`: Only scrape services that have a value of `true`
      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
      # to set this to `https` & most likely set the `tls_config` of the scrape config.
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: If the metrics are exposed on a different port to the
      # service then set this appropriately.
      - job_name: kubernetes-service-endpoints
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          # Relabel to scrape only endpoints that have
          # "prometheus.io/scrape = true" annotation.
          - action: keep
            regex: true
            source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
          # Relabel to configure scrape scheme for all service scrape targets
          # based on endpoints "prometheus.io/scheme = <scheme>" annotation.
          - action: replace
            regex: (https?)
            source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
            target_label: __scheme__
          # Relabel to customize metric path based on endpoints
          # "prometheus.io/path = <path>" annotation.
          - action: replace
            regex: (.+)
            source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
            target_label: __metrics_path__
          # Relabel to scrape only single, desired port for the service based
          # on endpoints "prometheus.io/port = <port>" annotation.
          - action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - action: replace
            source_labels: [__meta_kubernetes_namespace]
            target_label: kubernetes_namespace
          - action: replace
            source_labels: [__meta_kubernetes_service_name]
            target_label: kubernetes_name

      # Example scrape config for pods
      #
      # The relabeling allows the actual pod scrape endpoint to be configured via the
      # following annotations:
      #
      # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the
      # pod's declared ports (default is a port-free target if none are declared).
      - job_name: kubernetes-pods
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - action: keep
            regex: true
            source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
            action: replace
            target_label: __scheme__
            regex: (https?)
          - action: replace
            regex: (.+)
            source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            target_label: __metrics_path__
          - action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - action: replace
            source_labels: [__meta_kubernetes_namespace]
            target_label: kubernetes_namespace
          - action: replace
            source_labels: [__meta_kubernetes_pod_name]
            target_label: kubernetes_pod_name

      # Example scrape config for probing services via the Blackbox Exporter.
      #
      # The relabeling allows the actual service scrape endpoint to be configured
      # via the following annotations:
      #
      # * `prometheus.io/probe`: Only probe services that have a value of `true`
      - job_name: kubernetes-services
        kubernetes_sd_configs:
          - role: service
        metrics_path: /probe
        params:
          module:
            - http_2xx
        relabel_configs:
          - action: keep
            regex: true
            source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
          - source_labels: [__address__]
            target_label: __param_target
          - replacement: blackbox-exporter.monitoring.svc:9115
            target_label: __address__
          - source_labels: [__param_target]
            target_label: instance
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: kubernetes_name

      # Example scrape config for probing ingresses via the Blackbox Exporter.
      #
      # The relabeling allows the actual ingress scrape endpoint to be configured
      # via the following annotations:
      #
      # * `prometheus.io/probe`: Only probe services that have a value of `true`
      - job_name: kubernetes-ingresses
        kubernetes_sd_configs:
          - role: ingress
        metrics_path: /probe
        params:
          module:
            - http_2xx
        relabel_configs:
          - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path]
            regex: (.+);(.+);(.+)
            replacement: ${1}://${2}${3}
            target_label: __param_target
          - target_label: __address__
            replacement: blackbox-exporter.monitoring.svc:9115
          - source_labels: [__param_target]
            target_label: instance
          - action: labelmap
            regex: __meta_kubernetes_ingress_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_ingress_name]
            target_label: kubernetes_name

      - job_name: node-exporter
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names:
                - kube-system
        relabel_configs:
          - action: keep
            source_labels: [__meta_kubernetes_service_label_k8s_app]
            regex: node-exporter
          - action: keep
            source_labels: [__meta_kubernetes_endpoint_port_name]
            regex: metrics
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_service_name]
            target_label: service
          - source_labels: [__meta_kubernetes_pod_node_name]
            target_label: node
          - source_labels: [__meta_kubernetes_service_name]
            target_label: job
            replacement: ${1}
          - source_labels: [__meta_kubernetes_service_label_k8s_app]
            target_label: job
            regex: (.+)
            replacement: ${1}
          - target_label: endpoint
            replacement: metrics

      - job_name: kube-state-metrics
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names:
                - kube-system
        relabel_configs:
          - action: keep
            source_labels: [__meta_kubernetes_service_label_k8s_app]
            regex: kube-state-metrics
          - action: keep
            source_labels: [__meta_kubernetes_endpoint_port_name]
            regex: http-metrics
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_service_name]
            target_label: service
          - source_labels: [__meta_kubernetes_service_name]
            target_label: job
            replacement: ${1}
          - source_labels: [__meta_kubernetes_service_label_k8s_app]
            target_label: job
            regex: (.+)
            replacement: ${1}
          - target_label: endpoint
            replacement: http-metrics

      - job_name: kube-controller-manager
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names:
                - kube-system
        relabel_configs:
          - action: keep
            source_labels: [__meta_kubernetes_service_label_k8s_app]
            regex: kube-controller-manager
          - action: keep
            source_labels: [__meta_kubernetes_endpoint_port_name]
            regex: http-metrics
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_service_name]
            target_label: service
          - source_labels: [__meta_kubernetes_service_name]
            target_label: job
            replacement: ${1}
          - source_labels: [__meta_kubernetes_service_label_k8s_app]
            target_label: job
            regex: (.+)
            replacement: ${1}
          - target_label: endpoint
            replacement: http-metrics

      - job_name: kube-scheduler
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names:
                - kube-system
        relabel_configs:
          - action: keep
            source_labels: [__meta_kubernetes_service_label_k8s_app]
            regex: kube-scheduler
          - action: keep
            source_labels: [__meta_kubernetes_endpoint_port_name]
            regex: http-metrics
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_service_name]
            target_label: service
          - source_labels: [__meta_kubernetes_service_name]
            target_label: job
            replacement: ${1}
          - source_labels: [__meta_kubernetes_service_label_k8s_app]
            target_label: job
            regex: (.+)
            replacement: ${1}
          - target_label: endpoint
            replacement: http-metrics

      - job_name: kube-dns
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names:
                - kube-system
        relabel_configs:
          - action: keep
            source_labels: [__meta_kubernetes_service_label_k8s_app]
            regex: kube-dns
          - action: keep
            source_labels: [__meta_kubernetes_endpoint_port_name]
            regex: metrics
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_service_name]
            target_label: service
          - source_labels: [__meta_kubernetes_service_name]
            target_label: job
            replacement: ${1}
          - source_labels: [__meta_kubernetes_service_label_k8s_app]
            target_label: job
            regex: (.+)
            replacement: ${1}
          - target_label: endpoint
            replacement: metrics

