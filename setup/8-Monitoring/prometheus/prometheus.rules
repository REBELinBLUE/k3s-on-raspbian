groups:
  - name: Prometheus
    rules:
      - alert: Prometheus Config Reload Failed
        annotations:
          description: Reloading Prometheus' configuration has failed for {{ $labels.namespace }}/{{ $labels.pod }}
          summary: Reloading Prometheus' configuration failed
        expr: |
          prometheus_config_last_reload_successful{job="prometheus",namespace="monitoring"} == 0
        for: 10m
        labels:
          severity: warning

      - alert: Prometheus Notification Queue Running Full
        annotations:
          description: Prometheus' alert notification queue is running full for {{$labels.namespace}}/{{$labels.pod}}
          summary: Prometheus' alert notification queue is running full
        expr: |
          predict_linear(prometheus_notifications_queue_length{job="prometheus",namespace="monitoring"}[5m], 60 * 30) > prometheus_notifications_queue_capacity{job="prometheus",namespace="monitoring"}
        for: 10m
        labels:
          severity: warning

      - alert: Prometheus Error Sending Alerts
        annotations:
          description: Errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.Alertmanager}}
          summary: Errors while sending alert from Prometheus
        expr: |
          rate(prometheus_notifications_errors_total{job="prometheus",namespace="monitoring"}[5m]) / rate(prometheus_notifications_sent_total{job="prometheus",namespace="monitoring"}[5m]) > 0.01
        for: 10m
        labels:
          severity: warning

      - alert: Prometheus Error Sending Alerts
        annotations:
          description: Errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.Alertmanager}}
          summary: Errors while sending alerts from Prometheus
        expr: |
          rate(prometheus_notifications_errors_total{job="prometheus",namespace="monitoring"}[5m]) / rate(prometheus_notifications_sent_total{job="prometheus",namespace="monitoring"}[5m]) > 0.03
        for: 10m
        labels:
          severity: critical

      - alert: Prometheus not Connected to Alert Manager
        annotations:
          description: Prometheus {{ $labels.namespace }}/{{ $labels.pod}} is not connected to any Alert managers
          summary: Prometheus is not connected to any Alert managers
        expr: |
          prometheus_notifications_alertmanagers_discovered{job="prometheus",namespace="monitoring"} < 1
        for: 10m
        labels:
          severity: warning

      - alert: Prometheus TSDB Reloads Failing
        annotations:
          description: '{{$labels.job}} at {{$labels.instance}} had {{$value | humanize }} reload failures over the last four hours.'
          summary: Prometheus has issues reloading data blocks from disk
        expr: |
          increase(prometheus_tsdb_reloads_failures_total{job="prometheus",namespace="monitoring"}[2h]) > 0
        for: 12h
        labels:
          severity: warning

      - alert: Prometheus TSDB Compactions Failing
        annotations:
          description: '{{$labels.job}} at {{$labels.instance}} had {{$value | humanize }} compaction failures over the last four hours.'
          summary: Prometheus has issues compacting sample blocks
        expr: |
          increase(prometheus_tsdb_compactions_failed_total{job="prometheus",namespace="monitoring"}[2h]) > 0
        for: 12h
        labels:
          severity: warning

      - alert: Prometheus TSDB WAL Corruptions
        annotations:
          description: '{{$labels.job}} at {{$labels.instance}} has a corrupted write-ahead log (WAL).'
          summary: Prometheus write-ahead log is corrupted
        expr: |
          prometheus_tsdb_wal_corruptions_total{job="prometheus",namespace="monitoring"} > 0
        for: 4h
        labels:
          severity: warning

      - alert: Prometheus Not Ingesting Samples
        annotations:
          description: Prometheus {{ $labels.namespace }}/{{ $labels.pod}} isn't ingesting samples.
          summary: Prometheus isn't ingesting samples
        expr: |
          rate(prometheus_tsdb_head_samples_appended_total{job="prometheus",namespace="monitoring"}[5m]) <= 0
        for: 10m
        labels:
          severity: warning

      - alert: Prometheus Target Scrapes Duplicate
        annotations:
          description: '{{$labels.namespace}}/{{$labels.pod}} has many samples rejected due to duplicate timestamps but different values'
          summary: Prometheus has many samples rejected
        expr: |
          increase(prometheus_target_scrapes_sample_duplicate_timestamp_total{job="prometheus",namespace="monitoring"}[5m]) > 0
        for: 10m
        labels:
          severity: warning

  - name: Alert Manager
    rules:
      - alert: Alert Manager Failed Reload
        annotations:
          message: Reloading Alertmanager's configuration has failed for {{ $labels.namespace }}/{{ $labels.pod}}.
        expr: |
          alertmanager_config_last_reload_successful{job="alertmanager",namespace="monitoring"} == 0
        for: 10m
        labels:
          severity: warning
